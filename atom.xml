<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>marco Piccirilli</title>
    <description></description>
    <link>http://mpicci.github.io</link>
    <atom:link
      href="http://mpicci.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>CHOICE Heart Health Screening Event .</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;TODO&lt;/p&gt;
</description>
      <pubDate>
        Thu, 25 Oct 2018 00:00:00 -0400
      </pubDate>
      <link>http://mpicci.github.io/2018/10/25/CHOICE/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2018/10/25/CHOICE/</guid>
    </item>
    
    <item>
      <title>Voice Verification of similar speech.</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We want to study the problem of voice verification. The typical setup is
constituted by brief speech segments of different individuals. These
individuals have voices that sound alike. This is common in subjects
like twins. The system will learn the similarity metric between subjects
that are in same class for a subsequent verification step. In this
scenario the number of categories is very large and not known during
training, and the number of training samples for a single category is
very small. The learning process minimizes a discriminative loss
function that drives the similarity metric to be small for pairs of
features from similar subjects, and large for pairs from different
persons. The proposed architecture is a &lt;strong&gt;Siamese&lt;/strong&gt; network @Bromley93.
A general Siamese framework for visual recognition comprises two
identical networks and one cost module. The input to the system is a
pair of images and a label. The images are passed through the
sub-networks, yielding two outputs which are passed to the cost module
which produces the scalar energy. In speech recognition we have a
different type of signal. Usually the audio signal is 1-D, instead of
2-D as for the images. In this project we are going to use two
representation of the audio signal: &lt;strong&gt;spectrogram&lt;/strong&gt;, and &lt;strong&gt;MFCC&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;spectrogram&lt;/strong&gt; is a very detailed, accurate image of your audio,
displayed in either 2D or 3D. Audio is shown on a graph according to
time and frequency, with brightness or height indicating amplitude.
Whereas a waveform shows how your signal’s amplitude changes over time,
the spectrogram shows this change for every frequency component in the
signal.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;mel-frequency cepstrum MFC&lt;/strong&gt; is a representation of the
short-term power spectrum of a sound, based on a linear cosine transform
of a log power spectrum on a nonlinear mel scale of frequency
&lt;strong&gt;Mel-frequency cepstral coefficients MFCCs&lt;/strong&gt; are coefficients that
collectively make up an MFC. They are derived from a type of cepstral
representation of the audio clip, a nonlinear “spectrum-of-a-spectrum”.
The difference between the cepstrum and the mel-frequency cepstrum is
that in the &lt;strong&gt;MFC&lt;/strong&gt;, the frequency bands are equally spaced on the mel
scale, which approximates the human auditory system’s response more
closely than the linearly-spaced frequency bands used in the normal
cepstrum. This frequency warping can allow for better representation of
sound, for example, in audio compression.&lt;/p&gt;

&lt;p&gt;The difference of these two representation is the dimensionality. A
spectrogram is essentially an image, thus we can use the the well known
convolutional neural network used for computer vision applications. MFCC
constitute a 1-D vector that need to be analyzed by a neural network
with a slightly different architecture.&lt;/p&gt;

&lt;h3 id=&quot;a-more-advance-architecture-lstm&quot;&gt;A more advance architecture: LSTM&lt;/h3&gt;

&lt;p&gt;One of the reasons training networks is difficult is that the errors
computed in backpropagation are multiplied by each other once per
timestep. If the errors are small, the error quickly dies out, becoming
very small; if the errors are large, they quickly become very large due
to repeated multiplication. An alternative architecture built with Long
Short-Term Memory (LSTM) cells attempts to relieve from this issue.&lt;/p&gt;

&lt;p&gt;Deep LSTM and Bidirectional LSTM @Graves2013 were recently introduced to
speech recognition. These methods have several advantages: they do not
require forced alignments to pre-segment the acoustic data, they
directly optimise the probability of the target sequence conditioned on
the input sequence, and especially in the case of Sequence Transduction, they are able to learn an implicit language model from the
acoustic training data.&lt;/p&gt;

&lt;h2 id=&quot;dateset&quot;&gt;Dateset&lt;/h2&gt;

&lt;p&gt;Unfortunately, given the nature of this task, not so many dataset are
available with the required characteristic. For this project we use two
dataset. A detaset composed of a small number of subjects (24) uttering
digits, and a more large dataset composed of similar subjects voices in
a dialog. In this project we use two basic features: spectrogram, and
the “raw” soundwave. We focus on these basic representations because we
want to explore the studied architectures with very basic representation
and limited pre processing. More complex features can be used, however
we believe that convolutional layers with temporal based units like the
LSTM can achieve the state of the art performance.&lt;/p&gt;

&lt;p&gt;The larger dataset, that we call Speech dataset, is composed of 2057
files coming from roughly 300 subjects. Each recording is composed of a
brief dialog of 40 seconds acquired with different devices.&lt;/p&gt;

&lt;p&gt;Pre-processing&lt;/p&gt;

&lt;p&gt;An initial pre-processing stage has been applied to each recording to
eliminate the void spaces, to normalize the signal, and to resample at a
more lower common sample rate. Finally a conversion of the 16 bit signal
to the [-1,1] range has been applied due to better performance in the
network training.&lt;/p&gt;

&lt;p&gt;For the digit dataset we mainly used the spectrograms, obtained by a
Short Fourier Transform and the subsequent visualization of the
spectrum.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/Spectrum1.png&quot; alt=&quot;Spectrum&quot; width=&quot;250&quot; /&gt; 
&lt;img src=&quot;/data/2016-12-05/0_Agnes.wav.png&quot; width=&quot;250&quot; /&gt; 
&lt;/center&gt;
&lt;!------------------------------------------------------------------------------------------------------------------------------------------------&gt;
&lt;!-- ![Spectrogram: Speech dataset (Left), Digits dataset (Right).&lt;span data-label=&quot;fig:1&quot;&gt;&lt;/span&gt;](/data/2016-12-05/Spectrum1.png)          --&gt;
&lt;!-- ![Spectrogram: Speech dataset (Left), Digits dataset (Right).&lt;span data-label=&quot;fig:1&quot;&gt;&lt;/span&gt;](/data/2016-12-05/0_Agnes.wav.png &quot;fig:&quot;) --&gt;
&lt;!------------------------------------------------------------------------------------------------------------------------------------------------&gt;

&lt;h3 id=&quot;siamese-data&quot;&gt;Siamese data.&lt;/h3&gt;

&lt;p&gt;The siamese network @Bromley93 is composed of a feed forward network and
a siamese replica that shares the same weights. A downside of the
siamese framework is the higher number of samples require. In fact, each
sample is composed of a pair that can be from the same class ( label &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; )
or from different classes  ( label &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; ). To avoid the unbalance between
negative samples and positive we limit the number of total pairs by the
numbers of pairs that we can obtain from the same class. Since our
object is the subject verification, we divide the dataset in training
and testing sets by subjects. We select 400 subjects for training and
101 subjects for testing with the ratio $60.20 \%$. This division has been
done such that we have at lest a couple of sample for each subject.
Unfortunately there are subject with just one sample! Alternative will
be to use different temporal segments of the same sample as multiple
samples.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;

&lt;p&gt;We tested different architectures with the common feature to be a
siamese architecture. This feature is ideal to create a verification
scheme. We tested some naive configurations composed of a fully
connected structure working on spectrogram data. However the main focus
has been on a siamese LSTM.&lt;/p&gt;

&lt;h3 id=&quot;learning&quot;&gt;Learning&lt;/h3&gt;

&lt;p&gt;The learning of the siamese architecture can be quite challenging. We
refer to the work of Chopra et al @ChopraS2005 that present a similar
objective on face verification. The idea is to learn a function that
maps input patterns into a target space such that the $L_1$ norm in the
target space approximates the “semantic” distance in the input space.
The learning process minimizes a discriminative loss function that
drives the similarity metric to be small for pairs of faces from the
same person, and large for pairs from different persons.&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \mathcal{L}(W) = \sum_{i=1}^P L(W,(Y,X_1,X_2)^i)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
L(W,(Y,X_1,X_2)^i) = (1-Y) L_G (E_W(X_1,X_2)^i) + Y L_I (E_W(X_1,X_2)^i)
\end{equation}&lt;/p&gt;

&lt;p&gt;with 
\begin{equation}
  E_W(X_1,X_2) = ||G_W(X_1) - G_W(X_2)|| 
\end{equation}
&lt;script type=&quot;math/tex&quot;&gt;(Y,X_1,X_2)^i&lt;/script&gt; is the i-th sample which is composed of a pair of images and a label (genuine or
impostor), $L_G$ is the partial loss function for a genuine pair, $L_I$ the partial loss function for an impostor pair, and $P$ the number of
training samples. $L_I$ and $L_G$ should be designed in such a way that the minimization of $L$ will decrease the energy of genuine pairs and
increase the energy of impostor pairs.&lt;/p&gt;

&lt;h2 id=&quot;siamese-dense&quot;&gt;Siamese Dense&lt;/h2&gt;

&lt;p&gt;Since the proposed architecture (Siamese LSTM) is untested for audio
signals on the tested dataset, we used a fully connected architecture
(Figure [fig:net1]) as baseline method.&lt;/p&gt;

&lt;!-- &lt;center&gt; --&gt;
&lt;!-- &lt;img src=&#39;/data/2016-12-05/CPE691A_Report-figure0.png&#39; width=&quot;680px&quot;&gt; --&gt;
&lt;!-- &lt;/center&gt; --&gt;

&lt;h3 id=&quot;perfomance&quot;&gt;Perfomance&lt;/h3&gt;

&lt;p&gt;For this configuration we use the digit dataset with spectrogram
representation. Since the input layer is composed of fully connected
Relu units we vectorize the spectrogram images. For this experiment we
have 24 total subjects, with 19 used for training and the remaining for
testing.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/CPE691A_Report-figure0.png&quot; width=&quot;680px&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The average accuracy on the test set after 100 epochs is 54.79 %, and
56.87 % on the training set. For this configuration we use the
contrastive loss with L2 metric.&lt;/p&gt;

&lt;h2 id=&quot;soundnet-as-feature-extractor&quot;&gt;SoundNet as feature extractor.&lt;/h2&gt;

&lt;p&gt;To understand if a more deeper structure will be beneficial for the
siamese network we combine the feature extracted by the SoundNet
@Aytar2016 architecture with a dense siamese structure. In this case the
SoundNet is pretrained with a different dataset for a different
classification task as in @Aytar2016. We use this offline model to
extract the features. Although, the network is not finetuned for our
dataset we believe that the deeper structure can extract feature good
enough for the verification process. As we can see from the Figure
[fig:3] the performance outperform the baseline siamese network. The
average accuracy on the test set is 62.87 % and 65.38 % on the training
set.&lt;/p&gt;

&lt;p&gt;The performance of this&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet.png&quot; width=&quot;680px&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet_dense.png&quot; width=&quot;380px&quot; /&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet_siamese_acc.png&quot; width=&quot;380px&quot; /&gt;
&lt;/center&gt;

&lt;h1 id=&quot;siamese-convolutional-lstm&quot;&gt;Siamese Convolutional LSTM&lt;/h1&gt;
&lt;p&gt;The proposed architecture combine different structure in siamese
fashion. We want to take advantage of the Long-Short-Term-Memory unit
for its extraordinary performance on temporal data. LSTM is ideal for
time series data like sounds, because it can retain the important
information of the signal and forget pauses or unimportant data.
Unfortunately the raw audio signal can be too heavy to be directly
analyzed by the LSTM. Typical audio signals are sampled at 44100 Hz for
cd quality, and 16000 Hz for audio. LSTM are trainable with good
performance when the sequence is less than 300 sample. Unfortunately 300
samples at 16000 Hz is equivalent to 18 ms, that is quite short for
phoneme recognition. To create a low dimensional representation we
process the raw signal with one dimensional convolution, and one
dimensional MaxPooling. We show the network in Figure [fig:net3], and
the footprint on memory on Tables [tab:1],[tab:2]. The convolution
block preceding the LSTM will compress the long signal in a more concise
and richer feature set. There are two LSTM in cascade working
differently. The first one is letting the sequence pass to the second
LSTM but working like an accumulator, and memory storage. The second
LSTM instead will convert the sequence in a unique vector.&lt;/p&gt;

&lt;h2 id=&quot;network-configuration&quot;&gt;Network configuration&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/data/2016-12-05/Siamese_conv_LSTM2.png&quot; width=&quot;680px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One Leg Siamese configuration&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer type&lt;/th&gt;
      &lt;th&gt;Output shape&lt;/th&gt;
      &lt;th&gt;# Param&lt;/th&gt;
      &lt;th&gt;Connected to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 Conv1D 16 x 64&lt;/td&gt;
      &lt;td&gt;(6337,16)&lt;/td&gt;
      &lt;td&gt;1040&lt;/td&gt;
      &lt;td&gt;input&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 MaxPool1D 4&lt;/td&gt;
      &lt;td&gt;(1584,16)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L1 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 Conv1D 32 x 32&lt;/td&gt;
      &lt;td&gt;(1553,32)&lt;/td&gt;
      &lt;td&gt;16416&lt;/td&gt;
      &lt;td&gt;L1 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 MaxPool1D 4&lt;/td&gt;
      &lt;td&gt;(338,32)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L2 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L3 Conv1D 64 x 16&lt;/td&gt;
      &lt;td&gt;(373,64)&lt;/td&gt;
      &lt;td&gt;32832&lt;/td&gt;
      &lt;td&gt;L2 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L3 MaxPool1D 2&lt;/td&gt;
      &lt;td&gt;(186,64)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L3 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L4 Conv1D 128 x 8&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;65664&lt;/td&gt;
      &lt;td&gt;L3 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L5 LSTM 128 x 179&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;131584&lt;/td&gt;
      &lt;td&gt;L4 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L6 Dropout 0.5&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L5 LSTM&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L7 LSTM 128 x 1&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;131584&lt;/td&gt;
      &lt;td&gt;L6 Dropout&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L8 FC 128 x 1&lt;/td&gt;
      &lt;td&gt;(1, 128)&lt;/td&gt;
      &lt;td&gt;16512&lt;/td&gt;
      &lt;td&gt;L7 LSTM&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Siamese network configuration.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer type&lt;/th&gt;
      &lt;th&gt;Output shape&lt;/th&gt;
      &lt;th&gt;# Param&lt;/th&gt;
      &lt;th&gt;Connected to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Input 1&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input 2&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Conv Lstm&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;395632&lt;/td&gt;
      &lt;td&gt;input 1, input 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 metric&lt;/td&gt;
      &lt;td&gt;(1,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Conv Lstm 1, Conv Lstm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FC&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;129&lt;/td&gt;
      &lt;td&gt;L1 metric&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total # params:&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;395761&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;
&lt;p&gt;The network has been trained with the raw signal from the speech
dataset, as described above. In Figure [fig:5] we show the training
loss and accuracy on the train and validation data. We use rmsprop as
algorithm for training the network obtaining the accuracy of $~77 \%$ on
the validation set, and $98 \%$ on the training set.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SiameseConv_LSTM_loss.png&quot; width=&quot;380px&quot; /&gt;
&lt;img src=&quot;/data/2016-12-05/SiameseConv_LSTM_acc.png&quot; width=&quot;380px&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;We proved a slightly different architecture eliminating the fully
connected stage after the second LSTM obtaining the average accuracy of
roughly 84 % in the test set.&lt;/p&gt;
</description>
      <pubDate>
        Mon, 05 Dec 2016 00:00:00 -0500
      </pubDate>
      <link>http://mpicci.github.io/2016/12/05/CPE691A/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2016/12/05/CPE691A/</guid>
    </item>
    
  </channel>
</rss>
