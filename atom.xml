<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Marco Piccirilli</title>
    <description></description>
    <link>http://mpicci.github.io</link>
    <atom:link
      href="http://mpicci.github.io/atom.xml" rel="self"
      type="application/rss+xml" />
    
    <item>
      <title>Voice Verification of similar speech.</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We want to study the problem of voice verification. The typical setup is
constituted by brief speech segments of different individuals. These
individuals have voices that sound alike. This is common in subjects
like twins. The system will learn the similarity metric between subjects
that are in same class for a subsequent verification step. In this
scenario the number of categories is very large and not known during
training, and the number of training samples for a single category is
very small. The learning process minimizes a discriminative loss
function that drives the similarity metric to be small for pairs of
features from similar subjects, and large for pairs from different
persons. The proposed architecture is a &lt;strong&gt;Siamese&lt;/strong&gt; network @Bromley93.
A general Siamese framework for visual recognition comprises two
identical networks and one cost module. The input to the system is a
pair of images and a label. The images are passed through the
sub-networks, yielding two outputs which are passed to the cost module
which produces the scalar energy. In speech recognition we have a
different type of signal. Usually the audio signal is 1-D, instead of
2-D as for the images. In this project we are going to use two
representation of the audio signal: &lt;strong&gt;spectrogram&lt;/strong&gt;, and &lt;strong&gt;MFCC&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;spectrogram&lt;/strong&gt; is a very detailed, accurate image of your audio,
displayed in either 2D or 3D. Audio is shown on a graph according to
time and frequency, with brightness or height indicating amplitude.
Whereas a waveform shows how your signal’s amplitude changes over time,
the spectrogram shows this change for every frequency component in the
signal.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;mel-frequency cepstrum MFC&lt;/strong&gt; is a representation of the
short-term power spectrum of a sound, based on a linear cosine transform
of a log power spectrum on a nonlinear mel scale of frequency
&lt;strong&gt;Mel-frequency cepstral coefficients MFCCs&lt;/strong&gt; are coefficients that
collectively make up an MFC. They are derived from a type of cepstral
representation of the audio clip, a nonlinear “spectrum-of-a-spectrum”.
The difference between the cepstrum and the mel-frequency cepstrum is
that in the &lt;strong&gt;MFC&lt;/strong&gt;, the frequency bands are equally spaced on the mel
scale, which approximates the human auditory system’s response more
closely than the linearly-spaced frequency bands used in the normal
cepstrum. This frequency warping can allow for better representation of
sound, for example, in audio compression.&lt;/p&gt;

&lt;p&gt;The difference of these two representation is the dimensionality. A
spectrogram is essentially an image, thus we can use the the well known
convolutional neural network used for computer vision applications. MFCC
constitute a 1-D vector that need to be analyzed by a neural network
with a slightly different architecture.&lt;/p&gt;

&lt;h3 id=&quot;a-more-advance-architecture-lstm&quot;&gt;A more advance architecture: LSTM&lt;/h3&gt;

&lt;p&gt;One of the reasons training networks is difficult is that the errors
computed in backpropagation are multiplied by each other once per
timestep. If the errors are small, the error quickly dies out, becoming
very small; if the errors are large, they quickly become very large due
to repeated multiplication. An alternative architecture built with Long
Short-Term Memory (LSTM) cells attempts to relieve from this issue.&lt;/p&gt;

&lt;p&gt;Deep LSTM and Bidirectional LSTM @Graves2013 were recently introduced to
speech recognition. These methods have several advantages: they do not
require forced alignments to pre-segment the acoustic data, they
directly optimise the probability of the target sequence conditioned on
the input sequence, and especially in the case of Sequence Transduction, they are able to learn an implicit language model from the
acoustic training data.&lt;/p&gt;

&lt;h2 id=&quot;dateset&quot;&gt;Dateset&lt;/h2&gt;

&lt;p&gt;Unfortunately, given the nature of this task, not so many dataset are
available with the required characteristic. For this project we use two
dataset. A detaset composed of a small number of subjects (24) uttering
digits, and a more large dataset composed of similar subjects voices in
a dialog. In this project we use two basic features: spectrogram, and
the “raw” soundwave. We focus on these basic representations because we
want to explore the studied architectures with very basic representation
and limited pre processing. More complex features can be used, however
we believe that convolutional layers with temporal based units like the
LSTM can achieve the state of the art performance.&lt;/p&gt;

&lt;p&gt;The larger dataset, that we call Speech dataset, is composed of 2057
files coming from roughly 300 subjects. Each recording is composed of a
brief dialog of 40 seconds acquired with different devices.&lt;/p&gt;

&lt;p&gt;Pre-processing&lt;/p&gt;

&lt;p&gt;An initial pre-processing stage has been applied to each recording to
eliminate the void spaces, to normalize the signal, and to resample at a
more lower common sample rate. Finally a conversion of the 16 bit signal
to the [-1,1] range has been applied due to better performance in the
network training.&lt;/p&gt;

&lt;p&gt;For the digit dataset we mainly used the spectrograms, obtained by a
Short Fourier Transform and the subsequent visualization of the
spectrum.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/Spectrum1.png&quot; alt=&quot;Spectrum&quot; width=&quot;250&quot; /&gt; 
&lt;img src=&quot;/data/2016-12-05/0_Agnes.wav.png&quot; width=&quot;250&quot; /&gt; 
&lt;/center&gt;
&lt;!------------------------------------------------------------------------------------------------------------------------------------------------&gt;
&lt;!-- ![Spectrogram: Speech dataset (Left), Digits dataset (Right).&lt;span data-label=&quot;fig:1&quot;&gt;&lt;/span&gt;](/data/2016-12-05/Spectrum1.png)          --&gt;
&lt;!-- ![Spectrogram: Speech dataset (Left), Digits dataset (Right).&lt;span data-label=&quot;fig:1&quot;&gt;&lt;/span&gt;](/data/2016-12-05/0_Agnes.wav.png &quot;fig:&quot;) --&gt;
&lt;!------------------------------------------------------------------------------------------------------------------------------------------------&gt;

&lt;h3 id=&quot;siamese-data&quot;&gt;Siamese data.&lt;/h3&gt;

&lt;p&gt;The siamese network @Bromley93 is composed of a feed forward network and
a siamese replica that shares the same weights. A downside of the
siamese framework is the higher number of samples require. In fact, each
sample is composed of a pair that can be from the same class ( label &lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt; )
or from different classes  ( label &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; ). To avoid the unbalance between
negative samples and positive we limit the number of total pairs by the
numbers of pairs that we can obtain from the same class. Since our
object is the subject verification, we divide the dataset in training
and testing sets by subjects. We select 400 subjects for training and
101 subjects for testing with the ratio $60.20 \%$. This division has been
done such that we have at lest a couple of sample for each subject.
Unfortunately there are subject with just one sample! Alternative will
be to use different temporal segments of the same sample as multiple
samples.&lt;/p&gt;

&lt;h2 id=&quot;network-architecture&quot;&gt;Network Architecture&lt;/h2&gt;

&lt;p&gt;We tested different architectures with the common feature to be a
siamese architecture. This feature is ideal to create a verification
scheme. We tested some naive configurations composed of a fully
connected structure working on spectrogram data. However the main focus
has been on a siamese LSTM.&lt;/p&gt;

&lt;h3 id=&quot;learning&quot;&gt;Learning&lt;/h3&gt;

&lt;p&gt;The learning of the siamese architecture can be quite challenging. We
refer to the work of Chopra et al @ChopraS2005 that present a similar
objective on face verification. The idea is to learn a function that
maps input patterns into a target space such that the $L_1$ norm in the
target space approximates the “semantic” distance in the input space.
The learning process minimizes a discriminative loss function that
drives the similarity metric to be small for pairs of faces from the
same person, and large for pairs from different persons.&lt;/p&gt;

&lt;p&gt;\begin{equation}
  \mathcal{L}(W) = \sum_{i=1}^P L(W,(Y,X_1,X_2)^i)
\end{equation}&lt;/p&gt;

&lt;p&gt;\begin{equation}
L(W,(Y,X_1,X_2)^i) = (1-Y) L_G (E_W(X_1,X_2)^i) + Y L_I (E_W(X_1,X_2)^i)
\end{equation}&lt;/p&gt;

&lt;p&gt;with 
\begin{equation}
  E_W(X_1,X_2) = ||G_W(X_1) - G_W(X_2)|| 
\end{equation}
&lt;script type=&quot;math/tex&quot;&gt;(Y,X_1,X_2)^i&lt;/script&gt; is the i-th sample which is composed of a pair of images and a label (genuine or
impostor), $L_G$ is the partial loss function for a genuine pair, $L_I$ the partial loss function for an impostor pair, and $P$ the number of
training samples. $L_I$ and $L_G$ should be designed in such a way that the minimization of $L$ will decrease the energy of genuine pairs and
increase the energy of impostor pairs.&lt;/p&gt;

&lt;h2 id=&quot;siamese-dense&quot;&gt;Siamese Dense&lt;/h2&gt;

&lt;p&gt;Since the proposed architecture (Siamese LSTM) is untested for audio
signals on the tested dataset, we used a fully connected architecture
(Figure [fig:net1]) as baseline method.&lt;/p&gt;

&lt;!-- &lt;center&gt; --&gt;
&lt;!-- &lt;img src=&#39;/data/2016-12-05/CPE691A_Report-figure0.png&#39; width=&quot;680px&quot;&gt; --&gt;
&lt;!-- &lt;/center&gt; --&gt;

&lt;h3 id=&quot;perfomance&quot;&gt;Perfomance&lt;/h3&gt;

&lt;p&gt;For this configuration we use the digit dataset with spectrogram
representation. Since the input layer is composed of fully connected
Relu units we vectorize the spectrogram images. For this experiment we
have 24 total subjects, with 19 used for training and the remaining for
testing.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/CPE691A_Report-figure0.png&quot; width=&quot;680px&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The average accuracy on the test set after 100 epochs is 54.79 %, and
56.87 % on the training set. For this configuration we use the
contrastive loss with L2 metric.&lt;/p&gt;

&lt;h2 id=&quot;soundnet-as-feature-extractor&quot;&gt;SoundNet as feature extractor.&lt;/h2&gt;

&lt;p&gt;To understand if a more deeper structure will be beneficial for the
siamese network we combine the feature extracted by the SoundNet
@Aytar2016 architecture with a dense siamese structure. In this case the
SoundNet is pretrained with a different dataset for a different
classification task as in @Aytar2016. We use this offline model to
extract the features. Although, the network is not finetuned for our
dataset we believe that the deeper structure can extract feature good
enough for the verification process. As we can see from the Figure
[fig:3] the performance outperform the baseline siamese network. The
average accuracy on the test set is 62.87 % and 65.38 % on the training
set.&lt;/p&gt;

&lt;p&gt;The performance of this&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet.png&quot; width=&quot;680px&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet_dense.png&quot; width=&quot;380px&quot; /&gt;
&lt;img src=&quot;/data/2016-12-05/SoundNet_siamese_acc.png&quot; width=&quot;380px&quot; /&gt;
&lt;/center&gt;

&lt;h1 id=&quot;siamese-convolutional-lstm&quot;&gt;Siamese Convolutional LSTM&lt;/h1&gt;
&lt;p&gt;The proposed architecture combine different structure in siamese
fashion. We want to take advantage of the Long-Short-Term-Memory unit
for its extraordinary performance on temporal data. LSTM is ideal for
time series data like sounds, because it can retain the important
information of the signal and forget pauses or unimportant data.
Unfortunately the raw audio signal can be too heavy to be directly
analyzed by the LSTM. Typical audio signals are sampled at 44100 Hz for
cd quality, and 16000 Hz for audio. LSTM are trainable with good
performance when the sequence is less than 300 sample. Unfortunately 300
samples at 16000 Hz is equivalent to 18 ms, that is quite short for
phoneme recognition. To create a low dimensional representation we
process the raw signal with one dimensional convolution, and one
dimensional MaxPooling. We show the network in Figure [fig:net3], and
the footprint on memory on Tables [tab:1],[tab:2]. The convolution
block preceding the LSTM will compress the long signal in a more concise
and richer feature set. There are two LSTM in cascade working
differently. The first one is letting the sequence pass to the second
LSTM but working like an accumulator, and memory storage. The second
LSTM instead will convert the sequence in a unique vector.&lt;/p&gt;

&lt;h2 id=&quot;network-configuration&quot;&gt;Network configuration&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/data/2016-12-05/Siamese_conv_LSTM2.png&quot; width=&quot;680px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One Leg Siamese configuration&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer type&lt;/th&gt;
      &lt;th&gt;Output shape&lt;/th&gt;
      &lt;th&gt;# Param&lt;/th&gt;
      &lt;th&gt;Connected to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Input&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 Conv1D 16 x 64&lt;/td&gt;
      &lt;td&gt;(6337,16)&lt;/td&gt;
      &lt;td&gt;1040&lt;/td&gt;
      &lt;td&gt;input&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 MaxPool1D 4&lt;/td&gt;
      &lt;td&gt;(1584,16)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L1 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 Conv1D 32 x 32&lt;/td&gt;
      &lt;td&gt;(1553,32)&lt;/td&gt;
      &lt;td&gt;16416&lt;/td&gt;
      &lt;td&gt;L1 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L2 MaxPool1D 4&lt;/td&gt;
      &lt;td&gt;(338,32)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L2 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L3 Conv1D 64 x 16&lt;/td&gt;
      &lt;td&gt;(373,64)&lt;/td&gt;
      &lt;td&gt;32832&lt;/td&gt;
      &lt;td&gt;L2 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L3 MaxPool1D 2&lt;/td&gt;
      &lt;td&gt;(186,64)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L3 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L4 Conv1D 128 x 8&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;65664&lt;/td&gt;
      &lt;td&gt;L3 MaxPool1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L5 LSTM 128 x 179&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;131584&lt;/td&gt;
      &lt;td&gt;L4 Conv1D&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L6 Dropout 0.5&lt;/td&gt;
      &lt;td&gt;(179,128)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;L5 LSTM&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L7 LSTM 128 x 1&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;131584&lt;/td&gt;
      &lt;td&gt;L6 Dropout&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L8 FC 128 x 1&lt;/td&gt;
      &lt;td&gt;(1, 128)&lt;/td&gt;
      &lt;td&gt;16512&lt;/td&gt;
      &lt;td&gt;L7 LSTM&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Siamese network configuration.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Layer type&lt;/th&gt;
      &lt;th&gt;Output shape&lt;/th&gt;
      &lt;th&gt;# Param&lt;/th&gt;
      &lt;th&gt;Connected to&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Input 1&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input 2&lt;/td&gt;
      &lt;td&gt;(6400,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Conv Lstm&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;395632&lt;/td&gt;
      &lt;td&gt;input 1, input 2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;L1 metric&lt;/td&gt;
      &lt;td&gt;(1,1)&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;Conv Lstm 1, Conv Lstm&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FC&lt;/td&gt;
      &lt;td&gt;(1,128)&lt;/td&gt;
      &lt;td&gt;129&lt;/td&gt;
      &lt;td&gt;L1 metric&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Total # params:&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;395761&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;
&lt;p&gt;The network has been trained with the raw signal from the speech
dataset, as described above. In Figure [fig:5] we show the training
loss and accuracy on the train and validation data. We use rmsprop as
algorithm for training the network obtaining the accuracy of $~77 \%$ on
the validation set, and $98 \%$ on the training set.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2016-12-05/SiameseConv_LSTM_loss.png&quot; width=&quot;380px&quot; /&gt;
&lt;img src=&quot;/data/2016-12-05/SiameseConv_LSTM_acc.png&quot; width=&quot;380px&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;We proved a slightly different architecture eliminating the fully
connected stage after the second LSTM obtaining the average accuracy of
roughly 84 % in the test set.&lt;/p&gt;
</description>
      <pubDate>
        Mon, 05 Dec 2016 00:00:00 -0500
      </pubDate>
      <link>http://mpicci.github.io/2016/12/05/CPE691A/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2016/12/05/CPE691A/</guid>
    </item>
    
    <item>
      <title>A Mobile Structured Light System for 3D Face Acquisition</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h1 id=&quot;a-mobile-structured-light-system-for-3d-face-acquisition--writing-in-progress&quot;&gt;A Mobile Structured Light System for 3D Face Acquisition ( Writing in Progress!!!)&lt;/h1&gt;

&lt;h2 id=&quot;about-the-project&quot;&gt;About the project&lt;/h2&gt;

&lt;p&gt;In late 2012 I was asked to design a portable system able to scan human faces for border control.
At that time The Microsoft Kinect was dominating the field of the 3D acquisition devices, and in a very short time they sold more than 2 million devices.
Unfortunately Microsoft Kinect v1/v2 is not that friendly for very mobile applications, and more portable solutions, like project Tango, or sensor io were still in a sperimental stage.
After 5 years the scene is totally different, with &lt;a href=&quot;https://all3dp.com/1/best-3d-scanner-diy-handheld-app-software/&quot;&gt;dozen of devices available&lt;/a&gt;.
We were able to design an active stereo system composed of a mid-range smartphone, and a pico-projector, driven by an Android App that we wrote for the acquisition and 3D reconstruction.
In Spring 2013 The &lt;a href=&quot;https://citer.clarkson.edu/&quot;&gt;CiTer&lt;/a&gt; approved our project with a grant, and we deliver the outcomes in Spring 2014 at the CiTer Spring meeting at SUNY BUffalo (NY).
In that occasion we made a short demo about the device.
In 2016 our work was published on the &lt;a href=&quot;http://ieeexplore.ieee.org/document/7361976/&quot;&gt;IEEE Sensor Journal&lt;/a&gt;.&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/publications/Marco_cropped.png&quot; width=&quot;300px&quot; /&gt;
&lt;figcaption&gt;My 3D face.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;hardware-design&quot;&gt;Hardware Design&lt;/h1&gt;
&lt;p&gt;Since the device was targeted for biometric, in particular border control, portability, battery powered, speed, and accuracy were the driven factors.
For these reasons we excluded techniques based on multi views stereo that were giving good results on static objects &lt;a href=&quot;https://cvg.ethz.ch/mobile/LiveMetric3DReconstructionICCV2013.pdf&quot;&gt;ICCV13&lt;/a&gt;. 
We decided to use active scanning techniques, based on the illumination of the subject with structured light. Pretty well known are the work of &lt;a href=&quot;http://mesh.brown.edu/&quot;&gt;Taubin&lt;/a&gt; at Brown University, and &lt;a href=&quot;https://engineering.purdue.edu/ME/People/ptProfile?resource_id=117610&quot;&gt;Song Zhang&lt;/a&gt; at Purdue.
However, if the smartphones camera were good enough, the main problem was to find a compact and portable light source.
Fortunately, a new kind of devices were starting to be available at reasonable price, and battery powered: the &lt;a href=&quot;http://www.projectorreviews.com/projector-categories/pico-pocket-projectors/&quot;&gt;nano, pico, and micro projectors&lt;/a&gt;.
For our complete setup we decided to use an Android smartphone: a mid-range device &lt;a href=&quot;https://en.wikipedia.org/wiki/Nexus_4&quot;&gt;Nexus 4&lt;/a&gt; with Android &lt;a href=&quot;https://en.wikipedia.org/wiki/Android_Jelly_Bean&quot;&gt;JellyBean&lt;/a&gt;. Till Android Lollipop almost all the smartphone were capable to output the video signal from the charging port. After, with the advent of google chromecast they decided to exclude the video output, and now only a few devices can be connected through HDMI port.
However, right now there are smartphones and tablets with &lt;a href=&quot;http://www.laptopmag.com/articles/lenovo-yoga-tab3-pro&quot;&gt;included pico projectors&lt;/a&gt;.
Our prototype in figure was composed of the Nexus 4, fixed to the micro-projector by a common car holder, and connected via HDMI cable. Since the light from the micro projector can be too bright for the eye we decided to use an additional tele lens to avoid to be too close to the subject.
&lt;img src=&quot;/images/100_4116_scaled.JPG&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;active-stereo-reconstruction&quot;&gt;Active Stereo reconstruction&lt;/h1&gt;
&lt;p&gt;The pico projector and the smartphone constitute an &lt;a href=&quot;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwie1fHCqv_TAhXJYyYKHd7xBNQQFgg0MAE&amp;amp;url=http%3A%2F%2Fcvgl.stanford.edu%2Fteaching%2Fcs231a_winter1415%2Flecture%2Flecture8_volumetric_stereo_notes.pdf&amp;amp;usg=AFQjCNEQpGouAd7Szixwk-Io_kU6m4BwJg&amp;amp;sig2=auoQR3C1h_33Zb6rGFNNgg&quot;&gt;active stereo&lt;/a&gt; system, where, the pico projector substitute one of the camera in the stereo configuration.
This configuration permits to use the basic stereo formulation, but it’s more robust since less affected by the external light, serious problem for the stereo matching reconstruction. As will see later, using fringe pattern with active stereo configuration, speed up the scan, then the 3D reconstruction.
This solution, used by the majority of high precision 3D scanners present some downsides. The principal is the calibration procedure of the system.&lt;/p&gt;

&lt;h2 id=&quot;calibration&quot;&gt;Calibration&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&quot;https://boofcv.org/index.php?title=Tutorial_Camera_Calibration&quot;&gt;calibration&lt;/a&gt; of a camera lens is a very important step in 3D reconstruction, since other than the estimated ratio between the real and represented object, the lens introduce different distortions, making the reconstruction not accurate. With the calibration procedure we measure all the intrinsic and extrinsic quantities of the camera system, correcting distortions, and increasing the accuracy of the measures.
Traditional monocular optics can be easily calibrated using simple &lt;a href=&quot;http://www.vision.caltech.edu/bouguetj/calib_doc/&quot;&gt;algorithms&lt;/a&gt; that leverage the the knowledge of a given patterns and the relative representation on the focal plane in term of pixels. Common patterns are chessboards with black and white squares, thus easy to detect corners automatically.
For a multi-views system, other than the intrinsic parameters of each lens, wwe need to measure the position, and orientation, respect to a reference, of the cameras.
In the passive stereo, this is possible repeating the calibration procedure for each camera, then with the &lt;strong&gt;stereo registration&lt;/strong&gt; of the two acquired images is possible to compute the extrinsic, and parallax parameters.
In active stereo setup this procedure is quite hard, since the projector cannot &quot; see &quot; the pattern! The trick is to calibrate the camera first, then project the pattern with the projector and use the camera as a proxy. We refer to &lt;a href=&quot;http://ieeexplore.ieee.org/document/6375029/&quot;&gt;Moreno et. al.&lt;/a&gt; for a more extensive explanation and other references.&lt;/p&gt;

&lt;h1 id=&quot;three-phase-structured-light&quot;&gt;Three Phase Structured Light&lt;/h1&gt;
&lt;p&gt;When I started to study structured light for 3D active acquisition devices I was literally astonished to find so many parallel, and the same math  I was using a few years before in electrical communication. The same principle used in radar, sonar, and communication, is to transmit information &lt;code class=&quot;highlighter-rouge&quot;&gt;shape&lt;/code&gt; from the source: &lt;code class=&quot;highlighter-rouge&quot;&gt;object to scan&lt;/code&gt; to the receiver, the &lt;code class=&quot;highlighter-rouge&quot;&gt;camera&lt;/code&gt;.&lt;br /&gt;
However, the object per se do not emit any information. What the human eye, or a camera see is the appearance, something that is difficult to describe.
Varphilosopher, neuroscientists, and also computer scientist have often discussed about it, and it’s still an hot topic.&lt;/p&gt;

&lt;p&gt;To retrieve the shape &lt;em&gt;information&lt;/em&gt; we have to &lt;em&gt;sense&lt;/em&gt; the object. The information to acquire is in the form of x,y,z coordinate. Illuminating the subject with a light source, and receiving the distorted light with the camera is an analogous to radar, and sonar systems.
The difference is the frequency of the electromagnetic radiation used. Hundred, or thousand of MegaHertz in the case of the radar, 100 millions of Mhz for the visible light.&lt;br /&gt;
With different operating frequencies, although the same basic formulation, different noise and nuisances are playing a major role in the system performance.&lt;/p&gt;

&lt;p&gt;There have been many structured light works using different patterns. A primitive, but simple and thoughtful is the work of &lt;a href=&quot;http://www.vision.caltech.edu/bouguetj/ICCV98/&quot;&gt;Bouguet and Perona&lt;/a&gt;. They use a simple desk lamp, and by the object shadow is possible to reconstruct the shape.&lt;br /&gt;
However, the speed of the system is restricted by the camera frame rate, and the stick speed to create the shadow.
An extension to this principle is the use of more complex &lt;em&gt;structured&lt;/em&gt; patterns. The literature is vast, and is quite hard to keep track of all the little contributions.&lt;/p&gt;

&lt;p&gt;The three main typologies are: 
* binary coded light striping 
* gray/color coded light striping
* phase Shifting.&lt;/p&gt;

&lt;p&gt;The main difference regard the way the shape information is encoded.
A more thorough explanation can be found &lt;a href=&quot;http://www.sci.utah.edu/~gerig/CS6320-S2012/Materials/CS6320-CV-S2012-StructuredLight.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/Binary_coding.jpg&quot; width=&quot;500px&quot; /&gt;
&lt;/center&gt;

&lt;p&gt;The first two techniques code the amplitude of the pattern light with discrete values. A set of patterns are successively projected onto the measuring
surface, codeword for a given pixel is formed by a sequence of patterns.
The main problem with this techniques is that the number of patterns is limited by the codeword length. Gray code can be used for robustness: adjacent stripes must only differ in 1 bit. However these techniques, used in old scanners, make the system slow, and especially are not resilient to external illumination, because the information is coded in the white or black level of the light.&lt;/p&gt;

&lt;p&gt;A more interesting technique, introduced by &lt;a href=&quot;http://www.ifp.uni-stuttgart.de/publications/2001/Videometrics01-Guehring-4309-24.pdf&quot;&gt;Guehring et al&lt;/a&gt;
is the phase-shift PS method. The PS method projects a sequence of periodic intensity patterns, each of which is offset by a fraction of its period from the previous one, so that the entire period is covered. As a result, one obtains a so-called relative phase map, which is also of a periodic nature: values readily available from the relative &lt;em&gt;wrapped&lt;/em&gt; phase map are said to be wrapped in the range modulo &lt;script type=&quot;math/tex&quot;&gt;2 \pi&lt;/script&gt;.&lt;br /&gt;
The PS method typically assumes a projection of periodic sinusoidal patterns several times, where a periodic sine pattern is shifted between projections. A periodic pattern is actually shifted N   times by an amount of &lt;script type=&quot;math/tex&quot;&gt;\varphi_i&lt;/script&gt;, where shifts are equally distributed to cover the entire period:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi_i = \frac{2 \pi}{N} \cdot i \qquad i= 0,1,\dots,N-1&lt;/script&gt;

&lt;p&gt;For a camera image pixel, the detected gray-level intensity, &lt;script type=&quot;math/tex&quot;&gt;g(x,y)&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;(x,y)&lt;/script&gt; position, obtained as a result of a projected periodic pattern in context and for a shift i can be modeled as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g(x,y) = a(x,y) + b(x,y) \cdot \cos(2 \pi f_0 x + \varphi_i(x,y))&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;a(x,y)&lt;/script&gt; is the background intensity, &lt;script type=&quot;math/tex&quot;&gt;b(x,y)&lt;/script&gt; is the amplitude modulation of fringes, &lt;script type=&quot;math/tex&quot;&gt;f_0&lt;/script&gt; is the spatial carrier frequency, &lt;script type=&quot;math/tex&quot;&gt;\varphi_i(x,y)&lt;/script&gt;is the phase modulation of fringes.&lt;/p&gt;

&lt;h2 id=&quot;in-electrical-communications&quot;&gt;In Electrical Communications&lt;/h2&gt;
&lt;p&gt;The above equation is very common in electrical communications! It’s the basic formulation used in &lt;a href=&quot;https://en.wikibooks.org/wiki/Communication_Systems/What_is_Modulation%3F&quot;&gt;modulation theory&lt;/a&gt;.
In particular, the above form is called phase modulation, or angle modulation. This type of modulation is very interesting, since encode the information on phase variations.
Common distortions of the electrical signal on means like coaxial cables, fiber optics, or ether are principally associated with the amplitude, encoding the information on the phase makes the system more resilient to noise. 
Moreover, since &lt;script type=&quot;math/tex&quot;&gt;\varphi_i&lt;/script&gt; is discrete, the phase variations are discrete too. In electrical communications this modulation is called &lt;a href=&quot;https://en.wikipedia.org/wiki/Phase-shift_keying&quot;&gt;PSK: phase shift keying&lt;/a&gt;, used commonly to transmit digital signals.&lt;/p&gt;

&lt;p&gt;My experience&lt;/p&gt;

&lt;p&gt;As &lt;a href=&quot;https://en.wikipedia.org/wiki/Amateur_radio&quot;&gt;Ham Radio&lt;/a&gt; operator I used many many times the PSK for packet transmission. In early 2000 was able to meet and experimenting high speed packet transmission in Northern Italy and Slovenia where Matiaz Vidmar developed a BPSK radio at 2400 MHz.&lt;/p&gt;

&lt;h2 id=&quot;recover-the-relative-phase&quot;&gt;Recover the relative phase&lt;/h2&gt;
&lt;p&gt;Phase-shifting methods are extensively employed in optical metrology, especially with the development of digital computers and digital display technologies. A well known algorithm use just &lt;a href=&quot;https://www.osapublishing.org/ao/abstract.cfm?uri=ao-45-21-5086&quot;&gt;three phase shift&lt;/a&gt;. 
Three-step phase-shifting algorithm have the advantage of fast measurement because they require the minimum number of fringe images to reconstruct one 3D shape. Each shift has a variation of &lt;script type=&quot;math/tex&quot;&gt;2\pi/3&lt;/script&gt;. The correspondent intensities of fringe images are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
g_1(x,y) &amp;= a(x,y) + b(x,y) \cdot \cos(2 \pi f_0 x - 2\pi/3)\\
g_2(x,y) &amp;= a(x,y) + b(x,y) \cdot \cos(2 \pi f_0 x)\\
g_3(x,y) &amp;= a(x,y) + b(x,y) \cdot \cos(2 \pi f_0 x + 2\pi/3)\\
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;To recover the realtive phase there is a closed form solution:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi_R(x,y) = \arctan(\sqrt{3} \frac{g_1(x,y) - g_3(x,y)}{2g_2(x,y)-g_1(x,y)-g_3(x,y)})&lt;/script&gt;

&lt;h2 id=&quot;unwrapping&quot;&gt;Unwrapping&lt;/h2&gt;
&lt;p&gt;Due to the periodic nature of a given periodic pattern, &lt;script type=&quot;math/tex&quot;&gt;\varphi_R(x,y)&lt;/script&gt; value by itself is not a unique representative that we can use to solve the correspondence problem between the image pixels of two or more cameras, i.e., between a single camera and the source of projection, e.g. a common video projector.&lt;/p&gt;

&lt;!---------------------------------------&gt;
&lt;!-- &lt;img src=&#39;/images/phase_mod.png&#39;&gt; --&gt;
&lt;!---------------------------------------&gt;

&lt;p&gt;The phase &lt;script type=&quot;math/tex&quot;&gt;\varphi_R(x,y)&lt;/script&gt; is called modulo &lt;script type=&quot;math/tex&quot;&gt;2\pi&lt;/script&gt; at each pixel. If the fringe patterns contain multiple fringes, as often is the case, &lt;a href=&quot;http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471249351.html&quot;&gt;phase unwrapping&lt;/a&gt; is necessary to remove the sawtooth-like discontinuities and obtain a continuous phase map.
Some initial works on fringes used only one fringe containing the whole image. This technique lead to poor results due to the low resolution of the phase, or better said the degree/pixel ratio. Using more fringes we are able to have higher resolution, but with more fringes we have an increase of the computational complexity of the unwrapping algorithm.&lt;br /&gt;
A pattern period can be defined either directly by the number of requested periods that pattern must have, or by the length of a single period, bearing in mind the total available pattern width in the context.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/Unwrap.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure shows the appearance of two sine patterns defined by integer length periods &lt;script type=&quot;math/tex&quot;&gt;\lambda_1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda_2&lt;/script&gt;: each pattern column has an intensity according to the sine value of its position on the abscissa axis, and the propagation of the absolute phase axis along the width of the pattern. A pair of relative phases &lt;script type=&quot;math/tex&quot;&gt;(\varphi_{R,1}&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\varphi_{R,2})&lt;/script&gt; is indicated for the arbitrary absolute phase value &lt;script type=&quot;math/tex&quot;&gt;\varphi_{ABS}&lt;/script&gt; and the following equations hold:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi_{ABS} = k_1 \lambda_1 + \varphi_{R,1} = k_2 \lambda_2 +\varphi_{R,2}&lt;/script&gt;

&lt;p&gt;However, this method assume multiple patterns, and multiple acquisitions with different frequencies. Unfortunately, this could be a important downside
due to the low frame rate of the mobile device.&lt;/p&gt;

&lt;p&gt;In this project we considered a different &lt;strong&gt;unwrapping&lt;/strong&gt; technique, also known as &lt;strong&gt;spatial unwrapping&lt;/strong&gt;, instead of the one cited above, that is denominated &lt;strong&gt;frequency unwrapping&lt;/strong&gt;.
The main difference between the two techniques is the acquisition time. &lt;strong&gt;Spatial unwrapping&lt;/strong&gt; use (x,y) relation on the picture frame to reconstruct the absolute phase, instead &lt;strong&gt;frequency unwrapping&lt;/strong&gt; use additional fringe to construct the relation between the picture frame and the absolute phase.
The last techniques need to acquire additional frames that will cost in terms of acquisition time.
In our implementation, using this type of technique is not feasible due to the &lt;em&gt;mobile&lt;/em&gt; nature of the device. Making every acquisition blurry due to the hand and subjects movements. In fact they need to stay still for the whole acquisition.
Doing some math: at 30 fps, and using 3 phases, the acquisition will last &lt;script type=&quot;math/tex&quot;&gt;1/30 * 3 = 100&lt;/script&gt; ms. However, increasing the number of frames to 4 &lt;script type=&quot;math/tex&quot;&gt;1/30 * 4 = 133&lt;/script&gt; ms. These times are unfortunately high for a handheld device. Unfortunately we discover, as we’ll explain in major details in the next sections, that a smartphone is not the ideal device to grab and record frames.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;As a side note: the smartphone used for this project is the Nexus 4! I think more recent devices will definitely obtain better performances.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;fast-quality-guided-flood-fill-phase-unwrapping-algorithm&quot;&gt;Fast quality-guided flood-fill phase unwrapping algorithm&lt;/h1&gt;
&lt;p&gt;We used the &lt;a href=&quot;http://ro.uow.edu.au/cgi/viewcontent.cgi?article=10855&amp;amp;context=infopapers&quot;&gt;Fast quality-guided flood-fill phase unwrapping algorithm for three-dimensional fringe pattern profilometry&lt;/a&gt; by Chen et. al.
This algorithm present a good trade-off between speed, computatioanl complexity and results. As we’ll see later, not always perform well on human faces, and for this reason we modify it in a way we can correct the faulty reconstructions.&lt;/p&gt;

&lt;h2 id=&quot;description-by-the-authors&quot;&gt;Description by the authors:&lt;/h2&gt;
&lt;p&gt;The proposed method consists of three steps. First, after the acquisition of the wrapped phase &lt;script type=&quot;math/tex&quot;&gt;\varphi_R&lt;/script&gt;, a quality map is generated according to the phase variance adjacent pixels on the wrapped phase map. According to the quality map, the phase map is divided into several parts which are categorised as either rapid phase changing areas or smooth phase changing areas. Then quality-guided flood-fill phase unwrapping algorithm is applied to rapid phase changing areas and nonguided path-following algorithm is used in the smooth phase changing area. The proposed approach is much faster than the conventional non-guided path-following algorithm, and it is more robust than the non-guided path-following algorithm. Experiments are carried out to verify the performance.&lt;/p&gt;

&lt;p&gt;The following figures are obtained by our system projecting the patterns on a white projector screen.
In the first row there are the three shifted patterns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/1.png&quot; width=&quot;300&quot; /&gt; 
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/2.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/3.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/phasecrop.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/offsetcrop.png&quot; width=&quot;300&quot; /&gt; 
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Unwrapping/unwrappedcrop.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the second row: the reconstructed wrapped phase &lt;script type=&quot;math/tex&quot;&gt;\varphi_R&lt;/script&gt; on the left, an offset used by the unwrapping algorithm in center, and the final absolute phase &lt;script type=&quot;math/tex&quot;&gt;\varphi_{ABS}&lt;/script&gt; on the right.&lt;/p&gt;

&lt;!------------------------------------------------------------------------&gt;
&lt;!-- This *is*{:.underline} some `code`{:#id}{:.class}.                 --&gt;
&lt;!-- A [link](test.html){:rel=&#39;something&#39;} and some **tools**{:.tools}. --&gt;
&lt;!--      {:                                                              --&gt;
&lt;!-- &lt;u&gt;some text&lt;/u&gt;                                                   --&gt;
&lt;!------------------------------------------------------------------------&gt;

&lt;!---------------------------------------------------------------------------------&gt;
&lt;!-- $$                                                                          --&gt;
&lt;!-- \begin{align*}                                                              --&gt;
&lt;!--   &amp; \varphi(x,y) = \varphi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right) --&gt;
&lt;!--   = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \varphi(e_i, e_j) = \\                   --&gt;
&lt;!--   &amp; (x_1, \ldots, x_n) \left( \begin{array}{ccc}                            --&gt;
&lt;!--       \varphi(e_1, e_1) &amp; \cdots &amp; \varphi(e_1, e_n) \\                           --&gt;
&lt;!--       \vdots &amp; \ddots &amp; \vdots \\                                           --&gt;
&lt;!--       \varphi(e_n, e_1) &amp; \cdots &amp; \varphi(e_n, e_n)                              --&gt;
&lt;!--     \end{array} \right)                                                     --&gt;
&lt;!--   \left( \begin{array}{c}                                                   --&gt;
&lt;!--       y_1 \\                                                                --&gt;
&lt;!--       \vdots \\                                                             --&gt;
&lt;!--       y_n                                                                   --&gt;
&lt;!--     \end{array} \right)                                                     --&gt;
&lt;!-- \end{align*}                                                                --&gt;
&lt;!-- $$                                                                          --&gt;
&lt;!---------------------------------------------------------------------------------&gt;

&lt;h1 id=&quot;android-sdk-ndk-openframework&quot;&gt;Android SDK, NDK, OpenFramework&lt;/h1&gt;
&lt;p&gt;We used an unusual composition of tools and libraries to develop the software. &lt;a href=&quot;https://developer.android.com/studio/index.html&quot;&gt;Android SDK&lt;/a&gt; with the eclipse plugin was indispensable to create the apps, although painful and buggy at the time.
Then we decided to use &lt;a href=&quot;http://openframeworks.cc/&quot;&gt;OpenFramework&lt;/a&gt;, an open source library in C++ for creative coding. We decided to use this combination because OpenFramework contained a version for Android with some function useful for our development. Developed in C++, to create Android Apps we used the &lt;a href=&quot;https://developer.android.com/ndk/index.html&quot;&gt;Native Development Toolkit&lt;/a&gt;.In fact, the time we had to code everything and make a little in house acquisition was limited to one semester!!
OpenFramework contains simple GUI, and portions of other useful libraries, like &lt;a href=&quot;http://opencv.org/&quot;&gt;OpenCV&lt;/a&gt;, &lt;a href=&quot;https://www.khronos.org/opengles/&quot;&gt;OpenGL ES&lt;/a&gt;, &lt;a href=&quot;http://pointclouds.org/&quot;&gt;Point of Cloud&lt;/a&gt; library, and other useful utilities.&lt;/p&gt;

&lt;h2 id=&quot;the-apps&quot;&gt;The Apps&lt;/h2&gt;
&lt;p&gt;We developed initially three Android apps: calibration app, acquisition app, and reconstruction app. Later, we decided to include in a unique app the first two. Respective programs has been developed on PC too for correct some initials major bugs, and test the algorithms with a high resolution camera.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Despite a general shift towards remote cloud processing for a range of mobile applications, we argue that it is intrinsically desirable that heavy sensing tasks be carried out locally on-device, due to the usually tight latency requirements, and the prohibitively large data transmission requirement as dictated by the high Therefore we also demonstrate the feasibility of implementing and deploying the applications on mobile devices by showing low overhead for tasks like acquisition and 3D reconstruction, and more importantly visualization.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The main app, and also the most critical on a mobile deviceis the &lt;em&gt;acquisition&lt;/em&gt; app.
In fact, The acquisition is a temporized process, where the procedure is to simultaneously create and display the pattern, shoot with the camera, and
store on memory the image.&lt;/p&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Screenshot_2014-05-01-18-14-13.png&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Screenshot_2014-05-02-15-36-23_H.png&quot; width=&quot;320&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/Screenshot_2014-05-02-18-24-06.png&quot; width=&quot;300&quot; /&gt;
&lt;figcaption&gt;Left: acquisition app. Center: reconstruction and visualization app. Right: Android face detector in use.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;h2 id=&quot;the-hardware&quot;&gt;The Hardware&lt;/h2&gt;
&lt;p&gt;The hardware is composed of a Nexus 4 smartphone running Android 4.4, a pico-projector, and usb to HDMI dongle.
The pico-projector is battery powered and it can last one hour at full power. It’s able to acquire object at the distance of 2-3 meters on low light conditions, and 1-1.5 meters in normal light conditions..&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/100_4116.JPG&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/100_4117.JPG&quot; width=&quot;300&quot; /&gt;
&lt;img src=&quot;/images/publications/Smartphone_Sensor/100_4118.JPG&quot; width=&quot;535&quot; /&gt;
&lt;figcaption&gt;Smarthphone based 3D scanner.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;hr /&gt;
</description>
      <pubDate>
        Thu, 05 Jun 2014 00:00:00 -0400
      </pubDate>
      <link>http://mpicci.github.io/2014/06/05/3D_face/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2014/06/05/3D_face/</guid>
    </item>
    
    <item>
      <title>A framework to study the Human Body Surface Area</title>
      <description>&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
  tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML&quot;&gt;
&lt;/script&gt;

&lt;ul id=&quot;toc&quot;&gt;&lt;/ul&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;We present a virtual reality (VR) framework for the analysis of whole
human body surface area. Usual methods for determining the whole body
surface area (WBSA) are based on well known formulae, characterized by
large errors when the subject is obese, or belongs to certain
subgroups. For these situations, we believe that a computer vision
approach can overcome these problems and provide a better estimate of
this important body indicator.&lt;/p&gt;

&lt;p&gt;Unfortunately, using machine learning techniques to design a computer
vision system able to provide a new body indicator that goes beyond
the use of only body weight and height, entails a long and expensive
data acquisition process. A more viable solution is to use a dataset
composed of virtual subjects. Generating a virtual dataset allowed us
to build a population with different characteristics (obese,
underweight, age, gender). However, synthetic data might differ from a
real scenario, typical of the physician’s clinic. For this reason we
develop a new virtual environment to facilitate the analysis of human
subjects in 3D. This framework can simulate the acquisition process of
a real camera, making it easy to analyze and to create training data
for machine learning algorithms. With this virtual environment, we can
easily simulate the real setup of a clinic, where a subject is
standing in front of a camera, or may assume a different pose with
respect to the camera.&lt;/p&gt;

&lt;p&gt;We use this newly designated environment to analyze the whole body
surface area (WBSA). In particular, we show that we can obtain
accurate WBSA estimations with just one view, virtually enabling the
possibility to use inexpensive depth sensors (e.g., the Kinect) for
large scale quantification of the WBSA from a single view 3D map.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Accurate determination of the whole body surface area (WBSA) is one topic that has been actively studied over the last century. From the initial estimate of Du Bois and Du Bois&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; in $1916$ to recent work&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, and despite many critiques&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, the WBSA has attracted a lot of attention, driven primarily by the large variety of its applications.&lt;/p&gt;

&lt;h1 id=&quot;wbsa-measurements-and-estimation&quot;&gt;WBSA: Measurements and estimation&lt;/h1&gt;
&lt;p&gt;The common methods for WBSA calculation are through some well known formulae. The most widely used formula for WBSA calculation is the one devised by Du Bois and Du Bois in $1916$. Moulds of plaster of Paris for 9 subjects were cut into small pieces in an attempt to measure the two-dimensional surface area of the skin. Each individual’s body/skin surface area was then calculated and Du Bois and Du Bois determined that WBSA was related to stature and weight by the formula: $0.007184 \times W^{0.425} \times H^{0.725}$, where W is the weight (in kg) and H is the stature (in cm) of the subject. Notably, this formula was derived from $9$ subjects only, one of whom was a child.
Since the bodies of the subjects studied in the middle of the First World War are unlikely to be similar to the patients of the modern society, Mosteller&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; proposed a new calculation of WBSA in $1987$. This formula is a modification of the WBSA equation by Gehan and George&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Today there are many studies related to the verification of meaningful differences between WBSA measurements taken using a whole body three-dimensional (3D) scanner (criterion measure) and the estimates derived from each WBSA equation identified from systematic review&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. The 3D scanners used are often cumbersome and slow, and have to be operated by specially trained personnel.&lt;/p&gt;

&lt;h2 id=&quot;measurements-using-body-scanner&quot;&gt;Measurements using body scanner&lt;/h2&gt;
&lt;p&gt;An alternative to the use of WBSA formulae is whole-body 3D scanning. There are three major issues with the 3D laser scanners: cost, speed, and physical space requirement. Classic 3D laser scanners use a laser beam to illuminate the surface. At the same time a receptor registers the beam distortion on the surface and computes the respective depth. The beam needs to cover all the space of the surface and it takes time to do so. This requires that the object be almost immobile and small movements can cause errors in the reconstruction. Modern laser scanners are fast enough to avoid this distortion, but still require a large room to contain the device.&lt;/p&gt;

&lt;p&gt;The result of the scanning operation is usually ``raw’’ data in the form of a 3D $(x,y,z)$ point cloud. To reconstruct the mesh surface from the raw data, a surface reconstruction algorithm has to be applied. Without the face information it is not possible to relate the vertices to a face and thus compute the area of the surface. 
The 3D data, after surface reconstruction, is completed by other information than $(x,y,z)$ points. The reconstruction with triangles, for instance, fits many little triangles every $3$ points of the cloud. Then the calculation of the whole body surface area is reduced to a simple summation of the areas of all the triangles composing the mesh.
This solution, unfortunately, is not as reliable and efficient as it looks. Key challenges in 3D body scanning include occluded areas, body parts registration, device complexity and portability. Yu et al.&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; provide more detailed analysis on some of these problems.&lt;/p&gt;

&lt;h2 id=&quot;our-solution-virtual-subjects-and-virtual-environment&quot;&gt;Our solution: Virtual subjects and Virtual environment&lt;/h2&gt;
&lt;p&gt;Given these multiple problems, we decided to approach the WBSA calculation with an unusual methodology for this area.
Our goal can be summarized with the following idea. &lt;strong&gt;Using a simple Kinect device we want to obtain the accurate WBSA calculation of any given person regardless of differences in gender, race, obesity, with the subject simply facing the device without the supervision of a trained personnel.&lt;/strong&gt;
We want to use just one device that can acquire only one view of the subject, simplifying the setting required for an accurate estimation, and making possible the accurate estimation in a home setting.
The device will acquire just the visible portion of the body, and a subsequent prediction stage will reconstruct the overall WBSA.&lt;/p&gt;

&lt;p&gt;Unfortunately, the described system needs training data to be reliable, representing a large number of body shapes with significant diversity.
Since the collection of this large amount of data is expensive, time consuming, and very difficult, we decided to virtualize our training set, proposing a framework based on virtual subjects, computer vision and computer graphics techniques for the analysis and measurement of WBSA.&lt;/p&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/snapshot00.png&quot; alt=&quot;Raytracing.&quot; width=&quot;700&quot; /&gt; 
&lt;/center&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;D. Du Bois, E. F. Du Bois, D. Du Bois, and E. F. Du Bois, “A formula to estimate the approximate surface area if height and weight be known,” Nutrition, vol. 5, no. 5, pp. 303–311, 1989. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;N. Jander et al., “Indexing aortic valve area by body surface area increases the prevalence of severe aortic stenosis,” Heart, vol. 100, no. 1, pp. 28–33, Jan. 2014. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;M. Sawyer and M. J. Ratain, “Body surface area as a determinant of pharmacokinetics and drug dosing,” Invest New Drugs, vol. 19, no. 2, pp. 171–177, May 2001. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;R. D. Mosteller, “Simplified Calculation of Body-Surface Area,” N. Engl. J. Med., vol. 317, no. 17, pp. 1098–1098, Oct. 1987. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;E. A. Gehan and S. L. George, “Estimation of human body surface area from height and weight.,” Cancer Chemother. reports, vol. 54, no. 4, pp. 225–35, Aug. 1970. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;N. Daniell, T. Olds, and G. Tomkinson, “Technical note: Criterion validity of whole body surface area equations: a comparison using 3D laser scanning.,” Am. J. Phys. Anthropol., vol. 148, no. 1, pp. 148–55, May 2012. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;C. Y. Yu, C. H. Lin, and Y. H. Yang, “Human body surface area database and estimation formula,” Burns, vol. 36, no. 5, pp. 616–629, 2010. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
      <pubDate>
        Thu, 06 Jun 2013 00:00:00 -0400
      </pubDate>
      <link>http://mpicci.github.io/2013/06/06/BSA/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2013/06/06/BSA/</guid>
    </item>
    
    <item>
      <title>A Virtual Dataset of Human Bodies</title>
      <description>&lt;h1 id=&quot;some-antefacts&quot;&gt;Some antefacts&lt;/h1&gt;
&lt;p&gt;As everyone working on biometric and computer vision know the availability of data is fundamental for the research. For me, this was entirely new when in 2010 I started working on biometric.
Coming from a telecommunication environment, where we heavily rely on Montecarlo methods to simulate the channel distortion, the collection of real data was very new to me. 
Since the interest by the biometric community and mainly by the funding agencies (FBI, DoD), the analysis of the human shape analysis was my main research focus.
In 2011/2012 I was able to collect some interesting data with the Microsoft Kinect, and a NIR (Near Infrared) camera together with some anthropometric measurements, but the number was around 130 subjects with a prevalence of undergrad students and some adult.
These kind of collections are kinda expensive and time-consuming, two requisite that often are a big deal in small universities with a small budget, and students that need to finish for lack of funding.
Then autonomously I came out with an interesting solution able to keep my research going, and avoiding the change in my Ph.D. topic.&lt;/p&gt;

&lt;h1 id=&quot;body-models-and-human-shape-in-computer-vision-and-gaming&quot;&gt;Body models and Human shape in computer vision, and gaming&lt;/h1&gt;
&lt;p&gt;The human body is quite an interesting object, it’s composed of rigid and not rigid material, it has many degrees of freedom, and the appearance can change with age, gender, race, health status, and lifestyle. An enormous amount of works can be found in the modern and past literature. For instance, the Leonardo da Vinci &lt;a href=&quot;https://en.wikipedia.org/wiki/Vitruvian_Man&quot;&gt;Vitruvian man&lt;/a&gt;, just to cite one. In the last century, human body studies focused more on health assessment, or recognition, applying different statistical, and more physiological based techniques. However, in a data-driven approach, the necessity to collect human data for a statistically significant number of individuals is fundamental for avoiding biased results. The only dataset with a good amount of subjects is the &lt;a href=&quot;https://store.sae.org/caesar/&quot;&gt;CAESAR dataset&lt;/a&gt;. However, is not free, and the individuals in the dataset are only 2400.&lt;/p&gt;

&lt;p&gt;Although, we can leverage the &lt;a href=&quot;https://www.cdc.gov/nchs/nhanes/nhanes_questionnaires.htm&quot;&gt;NHANES&lt;/a&gt; dataset from the &lt;a href=&quot;https://www.cdc.gov/&quot;&gt;CDC&lt;/a&gt; that collects many anthropometric measurements and statistics of the American population for different years.
My initial thought was: if we can build bodies from these measurements we can replicate many analyses on real subjects with way less money and resources.
My study focus on works like &lt;a href=&quot;http://ai.stanford.edu/~drago/Projects/scape/scape.html&quot;&gt;SCAPE: Shape Completion and Animation of People&lt;/a&gt;, and &lt;a href=&quot;http://smpl.is.tue.mpg.de/&quot;&gt;skinned body models&lt;/a&gt;. Unfortunately, this was before the &lt;a href=&quot;https://ps.is.tuebingen.mpg.de/research_fields/shape&quot;&gt;enormous work&lt;/a&gt; done by &lt;a href=&quot;https://ps.is.tuebingen.mpg.de/person/black&quot;&gt;Michael Black&lt;/a&gt; in human shape, and human pose. 
However, Black’ goal were slightly different than mine, and the only relevant work for my research was the base for the launch of a startup, now acquired by Amazon.&lt;/p&gt;

&lt;p&gt;Another surprise came with the discovery of the character modeling for games. This area is definitely closer to the gaming and developer communities than scientific and research-based communities. I was attracted by the open source &lt;a href=&quot;http://www.makehumancommunity.org/&quot;&gt;MakeHuman&lt;/a&gt;. Before that, I was contemplating to use some commercial software, but the cost, the scarcity of funds (in those years I was close to getting unfounded! maybe I’ll write a blog….) made me lean toward the open source option where I could get my hands dirty with the source code.&lt;/p&gt;

&lt;h1 id=&quot;makehuman&quot;&gt;MakeHuman&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://www.makehumancommunity.org/&quot;&gt;Makehuman&lt;/a&gt; is an open source library from the prototyping of mesh characters. The contributions of many developers made MakeHuman extremely stable with multiple additions, and plugins.
Makehuman is unique because is written in python with the use of common python libraries, and just a few dependencies. The structure of the code is completely modular with the possibility to write additional plugins and classes without effort.&lt;/p&gt;

&lt;p&gt;Unfortunately MakeHuman has been design for the generation of a single character at the time, relying on other software for the animation.
However, I develop an efficient pipeline capable to generate many bodies automatically.&lt;/p&gt;

&lt;h1 id=&quot;body-generator&quot;&gt;Body Generator&lt;/h1&gt;
&lt;p&gt;The Main differences between my body generator and the other solutions (Shotton et al &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, Buys et al.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;) are two fundamental design goals.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The distribution of the generated population needs to be close enough to the distribution of real body population.
Shotton et al.’ generator focus on body poses, thus the goal is to generate mesh poses similar to real poses.
The generator uses the &lt;a href=&quot;http://mocap.cs.cmu.edu/&quot;&gt;CMU MoCAP&lt;/a&gt; dataset to re-target the mesh to a new pose. However, a limited number of subjects is used.
&lt;strong&gt;This makes the algorithm biased toward average size subjects&lt;/strong&gt;, as we can deduct from the Kinect v1 specs.
Our goal instead is to generate a large variety of body shapes when there are changes in anthropometric measurements and body composition.
We use the anthropometric measurements of real subject from the &lt;a href=&quot;https://www.cdc.gov/nchs/nhanes/nhanes_questionnaires.htm&quot;&gt;NHANES&lt;/a&gt; dataset as target distribution for the new population. We feed these measurements to the generator, represented by the buiding blocks in Figure.
A final measuring tool compare the anthropometric measurements on the generated population with the original population.
In a later iteration of the system we included a feedback loop to tune the body measurements closer to the original measurements.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
&lt;figure&gt;
&lt;img src=&quot;/data/2012-12-12/VirtualBody_pipeline.png&quot; alt=&quot;Pipeline&quot; width=&quot;700&quot; /&gt; 
&lt;figcaption&gt;Body Mesh Generator Pipeline.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;

&lt;ul&gt;
  &lt;li&gt;We don’t generate multiple body poses, but we have a virtual camera environment able to take multiple views of the body rendering.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h2 id=&quot;virtualbody-datasets-generated&quot;&gt;VirtualBody: Datasets Generated&lt;/h2&gt;
&lt;p&gt;We generated two datasets, which, can satisfy most of the real world scenarios.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Virtual NHANES Dataset:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Based on 500 real NHANES measurements, 12500 total shapes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;25 shapes for family: 5 values of weight, and 5 values of fat percentage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Each family: same stature, but very fine step variations in body shape.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Virtual Random Dataset:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;19995 shapes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The model parameters randomly chosen.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Defined distributions with defined ranges of values.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The result is a population quite vast in diversities.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/grab_2013-11-10_22.21.20.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; /&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/grab_2013-11-10_22.10.56.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; /&gt;
&lt;!-- &lt;img src=&#39;/data/2013-06-06-BSA/grab_2013-11-10_20.59.19.png&#39; alt=&quot;Raytracing.&quot;  width=&quot;150&quot; height=&quot;200&quot;&gt;  --&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/grab_2013-11-10_22.01.45.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; /&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/grab_2013-11-10_22.04.10.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; /&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/female_18.5BMI.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; height=&quot;225&quot; /&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/female_19.95BMI.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; height=&quot;225&quot; /&gt;
&lt;!-- &lt;img src=&#39;/data/2013-06-06-BSA/grab_2013-11-10_20.59.19.png&#39; alt=&quot;Raytracing.&quot;  width=&quot;150&quot; height=&quot;200&quot;&gt;  --&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/female_29.9BMI.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; height=&quot;225&quot; /&gt;
&lt;img src=&quot;/data/2013-06-06-BSA/female_38BMI.png&quot; alt=&quot;Raytracing.&quot; width=&quot;150&quot; height=&quot;225&quot; /&gt;
&lt;/center&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;J. Shotton et al., “Real-time human pose recognition in parts from single depth images,” in In In CVPR, 2011. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;D. Van Deun, V. Verhaert, K. Buys, B. Haex, and J. Vander Sloten, “Automatic generation of personalized human models based on body measurements,” 2011. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
      <pubDate>
        Wed, 12 Dec 2012 00:00:00 -0500
      </pubDate>
      <link>http://mpicci.github.io/2012/12/12/Virtual-bodies/</link>
      <guid isPermaLink="true">http://mpicci.github.io/2012/12/12/Virtual-bodies/</guid>
    </item>
    
  </channel>
</rss>
